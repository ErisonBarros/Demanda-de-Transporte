---
title: "Cluster Analysis"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      fig.path = "README_files/ClusterAnalysis/")
```

## Example Airports

> Your task: Create and evaluate the many types of clustering methods. 

### Variables: 

`Code`: Code of the airport;  

`Airport`: Name of the airport;  

`Ordem`: ID of the observations;  

`Passengers`: Number of passengers;  

`Movements`: Number of flights;  

`Numberofairlines`: Number of airlines in each airport;  

`Mainairlineflightspercentage`: Percentage of flights of the main airline of each airport;  

`Maximumpercentageoftrafficpercountry`: Maximum percentage of flights per country;  

`NumberofLCCflightsweekly`: Number of weekly low cost flights`;   

`NumberofLowCostAirlines`: Number of low cost airlines of each airport;  

`LowCostAirlinespercentage`: Percentage of the number of low cost airlines in each airport;  

`Destinations`: Number of flights arriving at each airport;  

`Average_route_Distance`: Average route distance in km;  

`DistancetoclosestAirport`: Distance to closest airport in km

`DistancetoclosestSimilarAirport`: Distance to closest similar airport in km;  

`AirportRegionalRelevance`: Relevance of the airport in a regional scale (0 - 1);  

`Distancetocitykm`: Distance between the airport and the city in km;  

`Inhabitantscorrected`: Population of the city;  

`numberofvisitorscorrected`: Number of vistors that arrived in the airport;  

`GDP corrected`: Corrected value of the Gross Domestic Product;  

`Cargoton`: Cargo ton. The total number of cargo transported in a certain period multiplied by the number o flights.


#### Import Libraries

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
library(readxl) # Library used for reading excel files
library(skimr) # Library used for summary statistics
library(tidyverse) # Library used in data science to perform exploratory data analysis
library(mclust) # Library used for model based clustering
library(cluster) # Library used for cluster analysis
library(factoextra) # Library used for visualizing distances
```

#### Set working directory

```{r}
setwd("G:/O meu disco/TDM - Lecture R/TDM github/Transport-Demand-Modelling/")
```

#### Import dataset

```{r}
dataset <- read_excel("Data/Data_Aeroports_Clustersv1.xlsX")
```

#### Transform dataset into dataframe

```{r}
df <- data.frame(dataset)
```

#### Summary statistics

```{r}
skim(df)
```

#### Now let us plot an example and take a look

```{r}
plot(Numberofairlines ~ Destinations, df)
with(df, text(Numberofairlines ~ Destinations, label = Airport, pos = 4, cex = 0.6))
```

> **Note:** You can already guess the number of clusters by visualizing the two variables. However, this is not clear and it does not consider the other variables in the analysis. 

#### Treat the data before performing a cluster analysis

* In this example we do not have missing values. In case you do have in the future, you can take out the missing values with listwise deletion.

```{r}
 df <- na.omit(df)
```

> **Note**: The listwise deletion removes the whole observation that has a missing value from the analysis. This may be appropriate only in some cases. There are many other forms of treating missing values.  

* Leave only continuous variables, and take out "Ordem"

```{r}
  drop <- c("Code","Airport", "Ordem")
  df_reduced = df[,!(names(df) %in% drop)]
```

> **Note:** We took out "Ordem" because it is an ID variable, and therefore it does not give value to the analysis.  

#### Take a look at the scale of the variables. See how they are different!
  
```{r}
head(df_reduced)
```

#### Z-score standardization - (xi - xmean / standard deviation)

```{r}
  mean <- apply(df_reduced, 2, mean) # The "2" in the function is used to select the columns. MARGIN: c(1,2)
  sd <- apply(df_reduced, 2, sd)
  df_scaled <- scale(df_reduced, mean, sd)
```
  
### HIERARCHICAL CLUSTERING

#### Measuring Similarity through Euclidean distances

```{r}
  distance <- dist(df_scaled, method = "euclidean")
```

> **Note:** There are other forms of distance measures that can be used such as: 
  i) Minkowski distance; 
  ii) Manhattan distance; 
  iii) Mahanalobis distance.
  
#### Visualize distances in heatmap

```{r}
  fviz_dist(distance, gradient = list(low = "#00AFBB", mid = "white", high = "#FC4E07"), order = FALSE)
```

#### Now, let us perform the many types of hierarchical clustering

**1. Single linkage (nearest neighbor) clustering algorithm**

Based on a bottom-up approach, by linking two clusters that have the closest distance between each other. 

```{r}
models <- hclust(distance, "single")
plot(models, labels = df$Airport, xlab = "Distance - Single linkage", hang = -1)

# Visualize the cut on the tree 
rect.hclust(models, 4, border = "purple") 
```


**2. Complete linkage (Farthest neighbor) clustering algorithm**

Complete linkage is based on the maximum distance between observations in each cluster.

```{r}
modelc <- hclust(distance, "complete")
plot(modelc, labels = df$Airport, xlab = "Distance - Complete linkage", hang = -1)

# Visualize the cut on the tree 
rect.hclust(modelc, 4, border = "blue") 
```

**3. Average linkage between groups** 

The average linkage considers the distance between clusters to be the average of the distances between observations in one cluster to all the members in the other cluster. 


```{r}
modela <- hclust(distance, "average")
plot(modela, labels = df$Airport, xlab = "Distance - Average linkage", hang = -1)
rect.hclust(modelc, 4, border = "red")
```

**4. Ward`s method**
  
The Ward`s method considers the measures of similarity as the sum of squares within the cluster summed over all variables. 


```{r}
modelw <- hclust(distance, "ward.D2")
plot(modelw, labels = df$Airport, xlab = "Distance - Ward method", hang = -1)
# Visualize where to cut on the tree (choose number of clusters)
rect.hclust(modelw, 4, border = "orange")
```

**5. Centroid method**

The centroid method considers the similarity between two clusters as the distance between its centroids.
  
```{r}
modelcen <- hclust(distance, "centroid")
plot(modelcen, labels = df$Airport, xlab = "Distance - Centroid method", hang = -1)
rect.hclust(modelcen, 4, border = "green")
```

#### Now lets evaluate the membership of each observation with the cutree function for each method.

```{r}
member_single <- cutree(models, 4)
member_com <- cutree(modelc, 4)
member_av <- cutree(modela, 4)
member_ward <- cutree(modelw, 4)
member_cen <- cutree(modelcen, 4)
```
  

#### Plot table to compare how common each method is to each other.

Let us compare the complete linkage with the average linkage

```{r}
table(member_com, member_av)
```

> **Note:** Try comparing other methods, and evaluate how common they are.   
  
#### Execute Silhouette Plots in order to evaluate which method is more appropriate.  

```{r}
plot(silhouette(member_single, distance))
plot(silhouette(member_com, distance))
plot(silhouette(member_av, distance))
plot(silhouette(member_ward, distance))
plot(silhouette(member_cen, distance))
```


>**Note:** The silhouette plot evaluates how similiar an observation is to its own cluster compared to other clusters. The clustering configuration is appropriate when most objects have high values. Low or negative values indicate that the clustering method is not appropriate or the number of clusters is not ideal. 

### NON-HiERARCHICAL CLUSTERING 

#### K-means clustering

```{r}
km_clust <- kmeans(df_scaled, 3)
```

Print out the results

```{r}
km_clust 

str(km_clust)

```


#### Choosing K 

#### This algorithm will detect how many clusters from 1 to 10 explains more variance
  
```{r}
  k <- list()
  for(i in 1:10){
    k[[i]] <- kmeans(df_scaled, i)
  }
```
  
> **Note**: Try printing the k value and take a look at the ratio "between_SS / total_SS". Evaluate how it varies when you add clusters. 

#### Now, let us try to plot (between_SS / total_SS) into a scree plot

```{r}
betSS_totSS <- list()
for(i in 1:10){
betSS_totSS[[i]] <- k[[i]]$betweenss/k[[i]]$totss
}
plot(1:10, betSS_totSS, type = "b", ylab = "Between SS / Total SS", xlab = "Number of clusters")
```


#### Let us try to take out the outliers and see the diference in the k-means clustering
 
* Examine the boxplots
  
```{r}
par(mar=c(15,2,1,1)) # Make labels fit in the boxplot
boxplot(df_scaled, las = 2)
```

* **Detect the outliers** 

```{r}
outliers <- boxplot.stats(df_scaled)$out

outliers
```

* **Remove rows with outliers** 

```{r}
df_no_outliers <- which(df_scaled %in% outliers,) 
```

> **Note:** There are many methods to treat outliers. This is just one of them. Note that it is not very appropriate, since it removes many observations that are relevant for the analysis. Try using other methods and evaluate the difference.  


#### Execute a k-means clustering with the dataset without the outliers and see the diference. 

```{r}
km_no_outliers <- kmeans(df_no_outliers, 3)

km_no_outliers
```


#### Finally, try plotting each variable with each other and analyze if the clusters make sense.  

Let us go back to first example and take a look. 

* **K-means with outliers**

```{r}
plot(Numberofairlines ~ Destinations, df, col = km_clust$cluster)
with(df, text(Numberofairlines ~ Destinations, label = Airport, pos = 1, cex = 0.6))
```

* **K-means without outliers**

```{r}
plot(Numberofairlines ~ Destinations, df, col = km_no_outliers$cluster)
with(df, text(Numberofairlines ~ Destinations, label = Airport, pos = 1, cex = 0.6))
```


